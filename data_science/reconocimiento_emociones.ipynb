{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagePaths= []\n",
      "1/1 [==============================] - 0s 482ms/step\n",
      "1/1 [==============================] - 0s 408ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los modelos de detección de puntos clave faciales y reconocimiento de emociones\n",
    "with open('detection.json', 'r') as json_file:\n",
    "    json_savedModel = json_file.read()\n",
    "\n",
    "model_1_facialKeyPoints = tf.keras.models.model_from_json(json_savedModel)\n",
    "model_1_facialKeyPoints.load_weights('weights_keypoint.hdf5')\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model_1_facialKeyPoints.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "with open('emotion.json', 'r') as json_file:\n",
    "    json_savedModel = json_file.read()\n",
    "\n",
    "model_2_emotion = tf.keras.models.model_from_json(json_savedModel)\n",
    "model_2_emotion.load_weights('weights_emotions.hdf5')\n",
    "model_2_emotion.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "dataPath = 'data'  # Cambia a la ruta donde hayas almacenado Data\n",
    "imagePaths = os.listdir(dataPath)\n",
    "print('imagePaths=', imagePaths)\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "faceClassif = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Crear una lista para almacenar las predicciones de emociones en varios cuadros\n",
    "emotion_predictions_buffer = []\n",
    "\n",
    "# Crear un DataFrame para almacenar las emociones predichas\n",
    "df_emociones = pd.DataFrame(columns=['Emocion'])\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    frame = cv2.flip(frame,1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    auxFrame = gray.copy()\n",
    "    nFrame = cv2.hconcat([frame, np.zeros((480, 300, 3), dtype=np.uint8)])\n",
    "\n",
    "    faces = faceClassif.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Obtener la región de interés (ROI) de la cara para detectar puntos clave faciales\n",
    "        face_roi = gray[y:y + h, x:x + w]\n",
    "        face_roi_resized = cv2.resize(face_roi, (96, 96), interpolation=cv2.INTER_CUBIC)\n",
    "        face_roi_normalized = face_roi_resized / 255.0\n",
    "        face_roi_reshaped = np.reshape(face_roi_normalized, (1, 96, 96, 1))\n",
    "\n",
    "        # Detección de puntos clave faciales\n",
    "        keypoints = model_1_facialKeyPoints.predict(face_roi_reshaped)\n",
    "\n",
    "        # Reconocimiento de emociones\n",
    "        emotion_prediction = model_2_emotion.predict(face_roi_reshaped)\n",
    "        emotion_class = np.argmax(emotion_prediction)\n",
    "\n",
    "        # Almacena la predicción en el buffer de predicciones\n",
    "        emotion_predictions_buffer.append(emotion_class)\n",
    "\n",
    "        # Limita el tamaño del buffer a un número determinado de cuadros\n",
    "        buffer_size = 10\n",
    "        if len(emotion_predictions_buffer) > buffer_size:\n",
    "            emotion_predictions_buffer.pop(0)\n",
    "\n",
    "        # Realiza el promedio de las predicciones en el buffer\n",
    "        average_emotion_prediction = np.mean(emotion_predictions_buffer)\n",
    "\n",
    "        # Emociones a detectar\n",
    "        emotions_labels = {0: 'Ira', 1: 'Odio', 2: 'Tristeza', 3: 'Felicidad', 4: 'Sorpresa'}\n",
    "\n",
    "        # Si la diferencia entre la predicción actual y el promedio es menor que un umbral, muestra la predicción promediada\n",
    "        threshold = 2\n",
    "        if len(set(emotion_predictions_buffer)) == 1 or abs(emotion_class - average_emotion_prediction) < threshold:\n",
    "            predicted_emotion = emotions_labels[int(round(average_emotion_prediction))]\n",
    "        else:\n",
    "            predicted_emotion = emotions_labels[emotion_class]\n",
    "\n",
    "        # Almacenar las emociones detectadas\n",
    "        df_emociones = pd.concat([df_emociones, pd.DataFrame({'Emocion': [predicted_emotion]})], ignore_index=True)\n",
    "        \n",
    "        # Mostrar la emoción predicha en la imagen\n",
    "        cv2.putText(frame, predicted_emotion, (x, y - 5), 1, 1.3, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Dibujar el rectángulo alrededor del rostro\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Mostrar la imagen con la emoción en tiempo real\n",
    "        nFrame = cv2.hconcat([frame, np.zeros((480, 300, 3), dtype=np.uint8)])\n",
    "\n",
    "    cv2.imshow('nFrame', nFrame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Emocion  Cantidad\n",
      "0  Felicidad        14\n",
      "1        Ira        13\n",
      "2       Odio         7\n",
      "3   Tristeza         7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Calcular la frecuencia de cada emoción\n",
    "emotion_counts = df_emociones['Emocion'].value_counts()\n",
    "\n",
    "# Crear el nuevo DataFrame\n",
    "df_emotion_summary = pd.DataFrame({\n",
    "    'Emocion': emotion_counts.index,\n",
    "    'Cantidad': emotion_counts.values\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por tipo de emoción (opcional)\n",
    "df_emotion_summary = df_emotion_summary.sort_values(by='Emocion')\n",
    "\n",
    "# Restablecer los índices del DataFrame\n",
    "df_emotion_summary = df_emotion_summary.reset_index(drop=True)\n",
    "\n",
    "# Exportar el DataFrame a un archivo JSON\n",
    "df_emotion_summary.to_json('emotion_summary.json', orient='records', indent=2)\n",
    "\n",
    "print(df_emotion_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
